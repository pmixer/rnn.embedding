# -*- coding: utf-8 -*-
"""CFIN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VfhzQ6FqaeIfwHoGuK1as2CQukc6ApE_

One of two baselines for MOOC dropout prediction

http://moocdata.cn/publications

https://github.com/wzfhaha/dropout_prediction
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd '/content/drive/My Drive/Colab Notebooks/6240/RethinkingRNN4NetworkEmbedding/cfin/data'
# %ls

import pandas as pd

# it's just parsed&joint json user logs
# test_log = pd.read_csv("./test_log.csv")

# extract feature from web log, actually it's just activity type based counting

# action types, recorded user's interaction with page DOMs on XuetangX
# actions = ['seek_video','play_video','pause_video','stop_video','load_video',
#       'problem_get','problem_check','problem_save','reset_problem',
#       'problem_check_correct', 'problem_check_incorrect',
#       'create_thread','create_comment','delete_thread','delete_comment',
#       'click_info','click_courseware','click_about','click_forum',
#       'click_progress', 'close_courseware']#video,problem,forum,click
import math
import numpy as np
import pandas as pd
import pickle as pkl
from sklearn.preprocessing import StandardScaler

# csv record course_id->user_id->session_id->list(activity, time)
train_log = pd.read_csv('train_log.csv')
test_log = pd.read_csv('test_log.csv')
all_log = pd.concat([train_log, test_log])

train_truth = pd.read_csv('train_truth.csv', index_col='enroll_id')
test_truth = pd.read_csv('test_truth.csv', index_col='enroll_id')
all_truth = pd.concat([train_truth, test_truth])

# enroll id as key for (course_id, user_id) pairs to connect tables
enroll_info = all_log[['username','course_id','enroll_id']].drop_duplicates()
enroll_info.index = enroll_info['enroll_id']
del enroll_info['enroll_id']

train_enroll = list(set(list(train_log['enroll_id'])))
test_enroll = list(set(list(test_log['enroll_id'])))

all_counts = all_log.groupby('enroll_id').count()
feature_mat = pd.DataFrame()
feature_mat['all#count'] = all_counts['action']
feature_mat['session#count'] = all_counts['session_id']

for a in all_log['action'].unique():
    feature_mat[a+"#num"] = 0

finer_counts = all_log.groupby(['enroll_id', 'action']).count()
for idx in finer_counts.index:
    enroll_id, action = idx
    feature_mat.loc[(enroll_id, action+"#num")] = finer_counts.loc[idx]['time']

# left_index=True, right_index=True means use index as key for join
feature_mat = pd.merge(feature_mat, all_truth, left_index=True, right_index=True)
feature_mat = pd.merge(feature_mat, enroll_info, left_index=True, right_index=True)

# feature_mat.loc[test_enroll].to_csv('test_features.csv')
# feature_mat.loc[train_enroll].to_csv('train_features.csv')
# train_feat= pd.read_csv('train_features.csv', index_col=0)
# test_feat= pd.read_csv('test_features.csv', index_col=0)
# all_feat = pd.concat([train_feat, test_feat])
all_feat = feature_mat
test_feat = feature_mat.loc[test_enroll]
train_feat = feature_mat.loc[train_enroll]

# adding more hand crafted features into design matrix
user_profile = pd.read_csv('user_info.csv', index_col='user_id')

# extract user age
birth_year = user_profile['birth'].to_dict()
def age_convert(y):
    if y == None or math.isnan(y):
        return 0
    a = 2018 - int(y)
    if a> 70 or a< 10: # hard coded age info
        a = 0
    return a
all_feat['age'] = [age_convert(birth_year.get(int(u),None)) for u in all_feat['username']]

# extract user gender
user_gender = user_profile['gender'].to_dict()
def gender_convert(g):
    if g == 'm':
        return 1
    elif g == 'f':
        return 2
    else:
        return 0
all_feat['gender'] = [gender_convert(user_gender.get(int(u),None)) for u in all_feat['username']]

user_edu = user_profile['education'].to_dict()
def edu_convert(x): # wried order, why not use dummy variable?
    edus = ["Bachelor's","High", "Master's", "Primary", "Middle","Associate","Doctorate"]
    #if x == None or or math.isnan(x):
    #    return 0
    if not isinstance(x, str):
        return 0
    ii = edus.index(x)
    return ii+1
all_feat['education'] = [edu_convert(user_edu.get(int(u), None)) for u in all_feat['username']]

user_enroll_num = all_feat.groupby('username').count()[['course_id']]
course_enroll_num = all_feat.groupby('course_id').count()[['username']]

user_enroll_num.columns = ['user_enroll_num']
course_enroll_num.columns = ['course_enroll_num']

all_feat = pd.merge(all_feat, user_enroll_num, left_on = 'username', right_index = True)
all_feat = pd.merge(all_feat, course_enroll_num, left_on='course_id', right_index=True)


#extract user cluster
cluster_label = pd.read_csv('cluster_label.csv', index_col='username')
all_feat = pd.merge(all_feat, cluster_label, left_on='username', right_index=True)


#extract course category
courseinfo = pd.read_csv('course_info.csv', index_col='id')
en_categorys = ['math','physics','electrical', 'computer','foreign language', 'business', 'economics','biology','medicine','literature','philosophy','history','social science', 'art','engineering','education','environment','chemistry']

def category_convert(cc):
    if isinstance(cc, str):
        for i, c in zip(range(len(en_categorys)), en_categorys):
            if cc == c:
                return i+1
    else:
        return 0
category_dict = courseinfo['category'].to_dict()

all_feat['course_category'] = [category_convert(category_dict.get(str(x), None)) for x in all_feat['course_id']]

act_feats = [c for c in train_feat.columns if 'count' in c or 'time' in c or 'num' in c]

num_feats = act_feats + ['age','course_enroll_num','user_enroll_num']
scaler= StandardScaler()
newX = scaler.fit_transform(all_feat[num_feats])

for i, n_f in enumerate(num_feats):
    all_feat[n_f] = newX[:,i]   

all_feat.loc[train_feat.index].to_csv('train_features.csv')
all_feat.loc[test_feat.index].to_csv('test_features.csv')

# start from here if you have got train/test features

import math
import numpy as np
import pandas as pd
import pickle as pkl
from sklearn.preprocessing import StandardScaler

train_feat= pd.read_csv('train_features.csv', index_col=0)
test_feat= pd.read_csv('test_features.csv', index_col=0)

# here's list of manually selected/generated features

act_feats = ['all#count',
 'session#count',
 'click_about#num',
 'click_info#num',
 'pause_video#num',
 'load_video#num',
 'play_video#num',
 'seek_video#num',
 'click_courseware#num',
 'close_courseware#num',
 'stop_video#num',
 'click_progress#num',
 'click_forum#num',
 'create_thread#num',
 'create_comment#num',
 'problem_get#num',
 'delete_thread#num',
 'problem_check#num',
 'problem_check_correct#num',
 'problem_check_incorrect#num',
 'problem_save#num',
 'delete_comment#num',
 'reset_problem#num',
 'close_forum#num']

# Commented out IPython magic to ensure Python compatibility.
# define the model and related methods

# %tensorflow_version 1.13

import numpy as np
from time import time
import tensorflow as tf
from sklearn.metrics import f1_score
from sklearn.metrics import roc_auc_score
from sklearn.base import BaseEstimator, TransformerMixin
from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm

class CFIN():
    def __init__(self, a_feat_size, u_feat_size, c_feat_size, a_field_size, u_field_size, c_field_size,
                 embedding_size=8,
                 conv_size = 32,
                 context_size = 16,
                 deep_layers=[32, 32], dropout_deep=[0.5, 0.5, 0.5], dropout_attn = [0.5, 0.5],
                 activation=tf.nn.relu,
                 attn_size = 16,
                 epoch=10, batch_size=256,
                 
                 learning_rate=0.001, optimizer_type="adam",
                 batch_norm=0, batch_norm_decay=0.995,
                 verbose=False, random_seed=2016,
                 loss_type="logloss", eval_metric=roc_auc_score,
                 l2_reg=0.0, attn=True):

        self.a_feat_size = a_feat_size      
        self.u_feat_size = u_feat_size
        self.c_feat_size = c_feat_size
        
        self.a_field_size = a_field_size
        self.u_field_size = u_field_size
        self.c_field_size = c_field_size
        self.conv_size = conv_size
        self.context_size = context_size
        self.embedding_size = embedding_size
         
        self.deep_layers = deep_layers
        self.dropout_deep = dropout_deep
        self.dropout_attn = dropout_attn
        self.activation = activation
        self.l2_reg = l2_reg
        self.attn_enable = attn
        self.epoch = epoch
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.optimizer_type = optimizer_type
        self.attn_size = attn_size
        self.batch_norm = batch_norm
        self.batch_norm_decay = batch_norm_decay
        self.greater_is_better = True 
        self.verbose = verbose
        self.random_seed = random_seed
        self.loss_type = loss_type
        self.eval_metric = eval_metric
        self.train_result, self.valid_result = [], []
        self._init_graph()
        

    def _init_graph(self):
        
        self.graph = tf.Graph()
        with self.graph.as_default():
            tf.set_random_seed(self.random_seed)
            
            self.u_feat_index = tf.placeholder(tf.int32, shape=[None, None],
                                                 name="u_feat_index")  # None * F
            self.c_feat_index = tf.placeholder(tf.int32, shape=[None, None], name='c_feat_index')
            self.a_feat_index = tf.placeholder(tf.int32, shape=[None, None], name='a_feat_index')
            self.dropout_keep_attn = tf.placeholder(tf.float32, shape=[None], name = 'dropout_attn_uc')
            
            self.u_feat_value = tf.placeholder(tf.float32, shape=[None, None], name="u_feat_value")
            self.c_feat_value = tf.placeholder(tf.float32, shape=[None, None], name="c_feat_value")
            self.a_feat_value = tf.placeholder(tf.float32, shape=[None, None], name="a_feat_value")
            
            self.label = tf.placeholder(tf.float32, shape=[None, 1], name="label")  # None * 1
            self.dropout_keep_deep = tf.placeholder(tf.float32, shape=[None], name="dropout_keep_deep")
            self.train_phase = tf.placeholder(tf.bool, name="train_phase")
            
            self.weights = self._initialize_weights()
             
            # model
            self.a_embeddings = tf.nn.embedding_lookup(self.weights["a_feat_embeddings"], self.a_feat_index)
            self.u_embeddings = tf.nn.embedding_lookup(self.weights['u_feat_embeddings'], self.u_feat_index)
            self.c_embeddings = tf.nn.embedding_lookup(self.weights['c_feat_embeddings'], self.c_feat_index)
                       
            u_feat_value = tf.reshape(self.u_feat_value, shape=[-1, self.u_field_size, 1])
            c_feat_value = tf.reshape(self.c_feat_value, shape=[-1, self.c_field_size, 1])
            a_feat_value = tf.reshape(self.a_feat_value, shape=[-1, self.a_field_size, 1])
            self.c_embeddings = tf.multiply(self.c_embeddings, c_feat_value)
            self.u_embeddings = tf.multiply(self.u_embeddings, u_feat_value)
            self.a_embeddings = tf.multiply(self.a_embeddings, a_feat_value)
            if self.batch_norm:
                self.a_embeddings = self.batch_norm_layer(self.a_embeddings, train_phase=self.train_phase, scope_bn='bn_conv')
            self.a_embeddings = tf.nn.conv1d(self.a_embeddings, self.weights['a_conv_filter'], stride=5, padding='VALID',data_format='NWC') + self.weights['a_conv_bias']
            """
            if self.batch_norm:
                self.a_embeddings = self.batch_norm_layer(self.a_embeddings, train_phase=self.train_phase, scope_bn='bn_conv')
            """
            self.a_embeddings = tf.nn.relu(self.a_embeddings)
            self.uc_embeddings = tf.concat([self.u_embeddings, self.c_embeddings], axis=1)
            
            self.uc_inter = tf.nn.relu(tf.matmul(tf.reshape(self.uc_embeddings, shape=[-1, (self.u_field_size+self.c_field_size)*self.embedding_size]), self.weights['ctx_pool_weight'])+self.weights['ctx_pool_bias'])

            self.uca_inter = tf.concat([tf.tile(tf.expand_dims(self.uc_inter, 1), [1,self.a_field_size//5,1]), self.a_embeddings], 2)
            
            self.attn_logit = tf.nn.relu(tf.matmul(tf.reshape(self.uca_inter, shape=[-1, self.conv_size + self.context_size]), self.weights['attn_out_1']) + self.weights['attn_bias_1'])
            self.attn_w = tf.nn.softmax(tf.reshape(tf.matmul(self.attn_logit, self.weights['attn_out']), shape=[-1, self.a_field_size//5]))
            if self.attn_enable:
                self.a_weight_emb = tf.multiply(tf.expand_dims(self.attn_w,2), self.a_embeddings)
            else:
                self.a_weight_emb = self.a_embeddings

            deep_input = tf.reduce_sum(self.a_weight_emb, axis=1)
            deep_input = tf.concat([deep_input, self.uc_inter], axis=1) 
            
            #self.y_deep = tf.reshape(deep_input, shape=[-1,  self.conv_size])
            self.y_deep = deep_input
            self.y_deep = tf.nn.dropout(self.y_deep, self.dropout_keep_deep[0])
            for i in range(0, len(self.deep_layers)):
                self.y_deep = tf.add(tf.matmul(self.y_deep, self.weights["layer_%d" %i]), self.weights["bias_%d"%i])
                if self.batch_norm:
                    self.y_deep = self.batch_norm_layer(self.y_deep, train_phase=self.train_phase, scope_bn="bn_%d" %i) 
                self.y_deep = self.activation(self.y_deep)
                self.y_deep = tf.nn.dropout(self.y_deep, self.dropout_keep_deep[1+i]) # dropout at each Deep layer
            
            self.out = tf.add(tf.matmul(self.y_deep, self.weights["logistic_weight"]), self.weights["logistic_bias"])

            # loss
            self.out = tf.nn.sigmoid(self.out)
            self.loss = tf.losses.log_loss(self.label, self.out)
            # l2 regularization on weights
            if self.l2_reg > 0:
                for k in self.weights.keys():
                    self.loss += tf.contrib.layers.l2_regularizer(
                            self.l2_reg)(self.weights[k])
            # optimizer
            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999,
                                                        epsilon=1e-8).minimize(self.loss)
            self.saver = tf.train.Saver()
            init = tf.global_variables_initializer()
            self.sess = self._init_session()
            self.sess.run(init)

            total_parameters = 0
            for variable in self.weights.values():
                shape = variable.get_shape()
                variable_parameters = 1
                for dim in shape:
                    variable_parameters *= dim.value
                total_parameters += variable_parameters
            if self.verbose > 0:
                print("#params: %d" % total_parameters)


    def _init_session(self):
        config = tf.ConfigProto()
        config.allow_soft_placement = True
        config.gpu_options.allow_growth = True
        config.gpu_options.per_process_gpu_memory_fraction = 0.9
        return tf.Session(config=config)

    def _init_layer_weight(self, input_size, output_size):
        glorot = np.sqrt(2.0 / (input_size + output_size))
        
        return np.random.normal(loc=0, scale=glorot, size=(input_size, output_size)), np.random.normal(loc=0, scale=glorot, size=(1, output_size))
 
    def _initialize_weights(self, c_node_embedding=None, u_node_embedding=None):
        weights = dict()

        # embeddings
        weights["a_feat_embeddings"] = tf.Variable(
            tf.random_normal([self.a_feat_size, self.embedding_size], 0.0, 0.1),
            name="a_feature_embeddings")  # feature_size * K

        weights["u_feat_embeddings"] = tf.Variable(
            tf.random_normal([self.u_feat_size, self.embedding_size], 0.0, 0.1),
            name="u_feature_embeddings")  # feature_size * K
        
        weights["c_feat_embeddings"] = tf.Variable(
            tf.random_normal([self.c_feat_size, self.embedding_size], 0.0, 0.1),
            name="c_feature_embeddings")  # feature_size * K
         
        u_pool_w, u_pool_b = self._init_layer_weight((self.u_field_size+self.c_field_size)*self.embedding_size, self.context_size)
        
        weights['ctx_pool_weight'] = tf.Variable(u_pool_w, name='ctx_pool_weight', dtype=np.float32)
        
        weights["ctx_pool_bias"] = tf.Variable(u_pool_b, name='ctx_pool_bias',dtype=np.float32)  # 1 * layers[0]
        
        a_conv_w, a_conv_b = self._init_layer_weight(5*self.embedding_size, self.conv_size)
        weights['a_conv_filter'] = tf.Variable(np.reshape(a_conv_w, [5, self.embedding_size, self.conv_size]), name='a_conv_filter', dtype=np.float32)
        weights['a_conv_bias'] = tf.Variable(np.reshape(a_conv_b, [1,1,self.conv_size]), name='a_conv_bias', dtype=np.float32)
        attn_out_1, attn_bias_1 = self._init_layer_weight(self.conv_size+self.context_size, self.attn_size)
        weights['attn_out_1'] = tf.Variable(attn_out_1, name='attn_out_1', dtype='float32')
        weights['attn_bias_1'] = tf.Variable(attn_bias_1, name='attn_bias_1', dtype='float32')
        attn_out, attn_bias = self._init_layer_weight(self.attn_size, 1)
        weights['attn_out'] = tf.Variable(attn_out, name='attn_out', dtype='float32')
        
        num_layer = len(self.deep_layers)
        input_size = self.conv_size + self.context_size
        
        glorot = np.sqrt(2.0 / (input_size + self.deep_layers[0]))
        weights["layer_0"] = tf.Variable(
            np.random.normal(loc=0, scale=glorot, size=(input_size, self.deep_layers[0])), dtype=np.float32)
        weights["bias_0"] = tf.Variable(np.random.normal(loc=0, scale=glorot, size=(1, self.deep_layers[0])), dtype=np.float32)  # 1 * layers[0]
        for i in range(1, num_layer):
            glorot = np.sqrt(2.0 / (self.deep_layers[i-1] + self.deep_layers[i]))
            weights["layer_%d" % i] = tf.Variable(
                np.random.normal(loc=0, scale=glorot, size=(self.deep_layers[i-1], self.deep_layers[i])),
                dtype=np.float32)  # layers[i-1] * layers[i]
            weights["bias_%d" % i] = tf.Variable(
                np.random.normal(loc=0, scale=glorot, size=(1, self.deep_layers[i])),
                dtype=np.float32)  # 1 * layer[i]
        
        input_size = self.deep_layers[-1]
        glorot = np.sqrt(2.0 / (input_size + 1))
        weights["logistic_weight"] = tf.Variable(
                        np.random.normal(loc=0, scale=glorot, size=(input_size, 1)),
                        dtype=np.float32)  # layers[i-1]*layers[i]
        weights["logistic_bias"] = tf.Variable(tf.constant(0.01), dtype=np.float32)
        return weights


    def batch_norm_layer(self, x, train_phase, scope_bn):
        bn_train = batch_norm(x, decay=self.batch_norm_decay, center=True, scale=True, updates_collections=None,
                              is_training=True, reuse=None, trainable=True, scope=scope_bn)
        bn_inference = batch_norm(x, decay=self.batch_norm_decay, center=True, scale=True, updates_collections=None,
                                  is_training=False, reuse=True, trainable=True, scope=scope_bn)
        z = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)
        return z


    def get_batch(self, ui, uv, ci, cv, ai, av, y, batch_size, index):
        start = index * batch_size
        end = (index+1) * batch_size
        end = end if end < len(y) else len(y)
        return ui[start:end], uv[start:end], ci[start:end], cv[start:end], ai[start:end], av[start:end], [[y_] for y_ in y[start:end]]

    def shuffle_in_unison_scary(self, a, b, c, d,e,f,g):
        rng_state = np.random.get_state()
        np.random.shuffle(a)
        np.random.set_state(rng_state)
        np.random.shuffle(b)
        np.random.set_state(rng_state)
        np.random.shuffle(c)
        np.random.set_state(rng_state)
        np.random.shuffle(d)
        np.random.set_state(rng_state)
        np.random.shuffle(e)
        np.random.set_state(rng_state)
        np.random.shuffle(f)
        np.random.set_state(rng_state)
        np.random.shuffle(g)

    def fit_on_batch(self, ui, uv, ci, cv, ai, av, y):
        feed_dict = {self.u_feat_index: ui,
                     self.u_feat_value: uv,
                     self.c_feat_index: ci,
                     self.c_feat_value: cv,
                     self.a_feat_index: ai,
                     self.a_feat_value: av,
                     self.dropout_keep_deep: self.dropout_deep,
                     self.dropout_keep_attn: self.dropout_attn,
                     self.label: y,
                     self.train_phase: True}
        loss, opt = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)
        return loss

    def fit(self, ui_train, uv_train, ci_train, cv_train, ai_train, av_train, y_train,
            ui_valid=None, uv_valid=None, ci_valid = None, cv_valid = None, ai_valid=None, av_valid=None,y_valid=None,
            early_stopping = True, early_stopping_round = 5, max_epoch =None):
        has_valid = uv_valid is not None
        best_epoch = 0
        if max_epoch:
            self.epoch = max_epoch
        for epoch in range(self.epoch):
            t1 = time()
            self.shuffle_in_unison_scary(ui_train, uv_train, ci_train, cv_train, ai_train, av_train, y_train)
            total_batch = int(len(y_train) / self.batch_size)
            for i in range(total_batch):
                ui_batch, uv_batch, ci_batch, cv_batch, ai_batch, av_batch, y_batch = self.get_batch(ui_train, uv_train, ci_train, cv_train, ai_train, av_train, y_train, self.batch_size, i)
                self.fit_on_batch(ui_batch, uv_batch, ci_batch, cv_batch, ai_batch, av_batch, y_batch)

            # evaluate training and validation datasets
            train_result, train_deep, train_f1 = self.evaluate(ui_train, uv_train, ci_train, cv_train, ai_train, av_train, y_train)
            self.train_result.append(train_result)
            if has_valid:
                valid_result, valid_deep, valid_f1= self.evaluate(ui_valid, uv_valid, ci_valid, cv_valid, ai_valid, av_valid, y_valid)
                self.valid_result.append(valid_result)
            if self.verbose > 0 and epoch % self.verbose == 0:
                if has_valid:
                    print("[%d] train-result=%.4f, valid-result=%.4f, valid-f1=%.4f [%.1f s]"
#                         % (epoch + 1, train_result, valid_result, valid_f1, time() - t1))
                else:
                    print("[%d] train-result=%.4f [%.1f s]"
#                         % (epoch + 1, train_result, time() - t1))
            
            if has_valid and early_stopping and self.training_termination(self.valid_result, early_stopping_round):
                best_epoch = epoch - early_stopping_round + 1
                print("best epoch: ", best_epoch)
                return best_epoch
                
        """         
        if has_valid and refit:
            if self.greater_is_better:
                best_valid_score = max(self.valid_result)
            else:
                best_valid_score = min(self.valid_result)
            best_epoch = self.valid_result.index(best_valid_score)
            best_train_score = self.train_result[best_epoch]
            ui_train = np.concatenate((ui_train, ui_valid), axis=0)
            uv_train = np.concatenate((uv_train, uv_valid), axis=0)
            ci_train = np.concatenate((ci_train, ci_valid), axis=0)
            cv_train = np.concatenate((cv_train, cv_valid), axis=0)
            ai_train = np.concatenate((ai_train, ai_valid), axis=0)
            av_train = np.concatenate((av_train, av_valid), axis=0)
            y_train = np.concatenate((y_train, y_valid), axis=0)

            for epoch in range(100):
                self.shuffle_in_unison_scary(ui_train, uv_train, ci_train, cv_train, ai_train, av_train, y_train)
                total_batch = int(len(y_train) / self.batch_size)
                for i in range(total_batch):
                    ui_batch, uv_batch, ci_batch, cv_batch, ai_batch, av_batch, y_batch = self.get_batch(ui_train, uv_train, ci_train, cv_train, ai_train, av_train, y_train, self.batch_size, i)
                    self.fit_on_batch(ui_batch, uv_batch, ci_batch, cv_batch, ai_batch, av_batch, y_batch)
                train_result, train_deep, train_f1 = self.evaluate(ui_train, uv_train, ci_train, cv_train, ai_train, av_train, y_train)
                if abs(train_result - best_train_score) < 0.001 or \
                    (self.greater_is_better and train_result > best_train_score) or \
                    ((not self.greater_is_better) and train_result < best_train_score):
                    break
        """
        save_path = self.saver.save(self.sess, "model/CFIN")
        print("Save to path: ", save_path)
        return save_path
        
    def training_termination(self, valid_result, early_stopping_round):
        if len(valid_result) > early_stopping_round:
            if self.greater_is_better:
                if max(valid_result[-early_stopping_round:]) > valid_result[-1-early_stopping_round]:
                    return False
                else:
                    return True
            else:
                if min(valid_result[-early_stopping_round:]) < valid_result[-1-early_stopping_round]:
                    return False
                else:
                    return True    
        return False

    def get_feats(self, ui,uv,ci,cv, ai, av, fname):
        dummy_y = [[1]] *len(ui)
        feed_dict = {self.u_feat_index: ui,
                     self.u_feat_value: uv,
                     self.c_feat_index: ci,
                     self.c_feat_value: cv,
                     self.a_feat_index: ai,
                     self.a_feat_value: av,
                     self.dropout_keep_deep: [1.0] * len(self.dropout_deep),
                     self.dropout_keep_attn: [1.0] * len(self.dropout_attn),
                     self.label: dummy_y,
                     self.train_phase: False}
        
        y_deep, y_rate= self.sess.run([self.y_deep, self.out], feed_dict = feed_dict)
        return y_deep,y_rate
    def get_attn(self, ui, uv, ci, cv, ai, av):
        dummy_y = [1] * len(ui)
        batch_index = 0
        ui_batch, uv_batch, ci_batch, cv_batch, ai_batch, av_batch ,y_batch = self.get_batch(ui, uv, ci, cv, ai, av, dummy_y, self.batch_size, batch_index)
        attn_weight = None
        uca_weight = None
        while len(ui_batch) > 0:
            num_batch = len(y_batch)
            feed_dict = {self.u_feat_index: ui_batch,
                         self.u_feat_value: uv_batch,
                         self.c_feat_index: ci_batch,
                         self.c_feat_value: cv_batch,
                         self.a_feat_index: ai_batch,
                         self.a_feat_value: av_batch,
                         self.dropout_keep_deep: [1.0] * len(self.dropout_deep),
                         self.dropout_keep_attn: [1.0] * len(self.dropout_attn),
                         self.label: y_batch,
                         self.train_phase: False}
            attn, uca_inter = self.sess.run([self.attn_w,self.uca_inter], feed_dict=feed_dict)
            if batch_index == 0:
                attn_weight = np.reshape(attn, (num_batch,-1))
                uca_weight = np.reshape(uca_inter, (num_batch, -1, self.embedding_size))
            else:
                attn_weight = np.concatenate((attn_weight, np.reshape(attn, (num_batch, -1))))
                uca_weight = np.concatenate((uca_weight, np.reshape(uca_inter, (num_batch, -1, self.embedding_size))))
            batch_index += 1
            
            ui_batch, uv_batch, ci_batch, cv_batch, ai_batch, av_batch ,y_batch = self.get_batch(ui, uv, ci, cv, ai, av, dummy_y, self.batch_size, batch_index)
        return attn_weight, uca_weight

   
    def predict(self, ui, uv, ci, cv, ai, av):
        dummy_y = [1] * len(ui)
        batch_index = 0
        ui_batch, uv_batch, ci_batch, cv_batch, ai_batch, av_batch ,y_batch = self.get_batch(ui, uv, ci, cv, ai, av, dummy_y, self.batch_size, batch_index)
        y_pred = None
        while len(ui_batch) > 0:
            num_batch = len(y_batch)
            feed_dict = {self.u_feat_index: ui_batch,
                         self.u_feat_value: uv_batch,
                         self.c_feat_index: ci_batch,
                         self.c_feat_value: cv_batch,
                         self.a_feat_index: ai_batch,
                         self.a_feat_value: av_batch,
                         self.dropout_keep_deep: [1.0] * len(self.dropout_deep),
                         self.dropout_keep_attn: [1.0] * len(self.dropout_attn),
                         self.label: y_batch,
                         self.train_phase: False}
            y_deep,batch_out = self.sess.run([self.y_deep,self.out], feed_dict=feed_dict)
            if batch_index == 0:
                y_pred = np.reshape(batch_out, (num_batch,))
            else:
                y_pred = np.concatenate((y_pred, np.reshape(batch_out, (num_batch,))))

            batch_index += 1
            
            ui_batch, uv_batch, ci_batch, cv_batch, ai_batch, av_batch ,y_batch = self.get_batch(ui, uv, ci, cv, ai, av, dummy_y, self.batch_size, batch_index)
        return y_deep,y_pred


    def evaluate(self, ui, uv, ci, cv, ai, av, y):
        y_deep, y_pred = self.predict(ui, uv, ci, cv, ai, av)  
        return self.eval_metric(y, y_pred), y_deep, f1_score(y, [1 if x>0.5 else 0 for x in y_pred])

# Commented out IPython magic to ensure Python compatibility.
# training and testing
# %timeit

import os
import sys
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.metrics import roc_auc_score

act_feat_basic = act_feats
user_cat_feat = ['gender', 'cluster_label']
user_num_feat = ['age','user_enroll_num']
course_cat_feat = ['course_category']
course_num_feat = ['course_enroll_num']

def feat_augment(train_feat, test_feat):
    act_feats = act_feat_basic
    all_feat = pd.concat([train_feat, test_feat])
    all_feat_u_mean = all_feat.groupby('username').mean()[act_feats]
    all_feat_u_mean.columns = [x+'#user#mean' for x in all_feat_u_mean.columns]
    all_feat_u_max = all_feat.groupby('username').max()[act_feats]
    all_feat_u_max.columns = [x+'#user#max' for x in all_feat_u_max.columns]

    all_feat = pd.merge(all_feat, all_feat_u_mean, right_index=True, left_on='username')
    all_feat = pd.merge(all_feat, all_feat_u_max, right_index=True, left_on='username')

    all_feat_c_mean = all_feat.groupby('course_id').mean()[act_feats]
    all_feat_c_mean.columns = [x+'#course#mean' for x in all_feat_c_mean.columns] 

    all_feat_c_max = all_feat.groupby('course_id').max()[act_feats]
    all_feat_c_max.columns = [x+'#course#max' for x in all_feat_c_max.columns]

    all_feat = pd.merge(all_feat, all_feat_c_mean, right_index=True, left_on='course_id')
    all_feat = pd.merge(all_feat, all_feat_c_max, right_index=True, left_on='course_id')
    
    act_feat = []
    for f in act_feat_basic:
        act_feat.append(f)
        act_feat.append(f+'#user#mean')
        act_feat.append(f+'#user#max')
        act_feat.append(f+'#course#mean')
        act_feat.append(f+'#course#max')

    return all_feat.loc[train_feat.index], all_feat.loc[test_feat.index], act_feat

def dataparse(train, test, num_feat, cat_feat):
    all_data = pd.concat([train, test])
    feat_dim = 0
    feat_dict = dict()
    for f in cat_feat:
        cat_val = all_data[f].unique()
        feat_dict[f] = dict(zip(cat_val, range(feat_dim, len(cat_val)+feat_dim)))
        feat_dim += len(cat_val)
    for f in num_feat:
        feat_dict[f] = feat_dim
        feat_dim += 1
    data_indice = all_data.copy()
    data_value = all_data.copy()
    for f in all_data.columns:
        if f in num_feat:
            data_indice[f] = feat_dict[f]
        elif f in cat_feat:
            data_indice[f] = data_indice[f].map(feat_dict[f])
            data_value[f] = 1.
        else:
            data_indice.drop(f, axis=1, inplace=True)
            data_value.drop(f, axis=1, inplace=True) 
    return feat_dim, data_indice, data_value

def load_data():
    dfTrain = pd.read_csv('train_features.csv', index_col=0)
    dfTest = pd.read_csv('test_features.csv', index_col=0)
    dfTrain_u = dfTrain[user_num_feat + user_cat_feat +['truth']]
    dfTest_u = dfTest[user_num_feat + user_cat_feat +['truth']]
    
    dfTrain_c = dfTrain[course_num_feat + course_cat_feat +['truth']]
    dfTest_c = dfTest[course_num_feat + course_cat_feat +['truth']]
    dfTrain_a = dfTrain[act_feat_basic +['truth','username','course_id']]
    dfTest_a = dfTest[act_feat_basic +['truth','username','course_id']]
    
    return dfTrain_u, dfTest_u, dfTrain_c, dfTest_c, dfTrain_a, dfTest_a


def model_run(dfTrain_u, dfTest_u, dfTrain_c, dfTest_c, dfTrain_a, dfTest_a, params, act_feat):
        
    u_feat_dim, u_data_indice, u_data_value = dataparse(dfTrain_u, dfTest_u, user_num_feat, user_cat_feat)
    ui_train, uv_train = np.asarray(u_data_indice.loc[dfTrain_u.index], dtype=int), np.asarray(u_data_value.loc[dfTrain_u.index], dtype=float)
    ui_test, uv_test = np.asarray(u_data_indice.loc[dfTest_u.index], dtype=int), np.asarray(u_data_value.loc[dfTest_u.index], dtype=float)
    
    params["u_feat_size"] = u_feat_dim
    params["u_field_size"] = len(ui_train[0])
        
    c_feat_dim, c_data_indice, c_data_value = dataparse(dfTrain_c, dfTest_c, course_num_feat, course_cat_feat)
    ci_train, cv_train = np.asarray(c_data_indice.loc[dfTrain_c.index], dtype=int), np.asarray(c_data_value.loc[dfTrain_c.index], dtype=float)
    ci_test, cv_test = np.asarray(c_data_indice.loc[dfTest_c.index], dtype=int), np.asarray(c_data_value.loc[dfTest_c.index], dtype=float)
    
    params["c_feat_size"] = c_feat_dim
    params["c_field_size"] = len(ci_train[0])
    
    av_train = np.asarray(dfTrain_a[act_feat], dtype=float)
    
    ai_train = np.asarray([range(len(act_feat)) for x in range(len(dfTrain_a))], dtype=int)
    av_test = np.asarray(dfTest_a[act_feat], dtype=float)
    ai_test = np.asarray([range(len(act_feat)) for x in range(len(dfTest_a))], dtype=int)

    params["a_feat_size"] = len(ai_train[0])
    params["a_field_size"] = len(ai_train[0])
    
    y_train = np.asarray(dfTrain_a['truth'], dtype=int)
    y_test = np.asarray(dfTest_a['truth'], dtype=int)
    model = CFIN(**params)
   
    # generate valid set
    train_num = len(y_train)
    indices = np.arange(train_num)
    np.random.shuffle(indices)
    split_ = int(0.8*train_num)
    ui_train_, ui_valid = ui_train[indices][:split_],ui_train[indices[split_:]]
    uv_train_, uv_valid = uv_train[indices][:split_],uv_train[indices[split_:]]
    ci_train_, ci_valid = ci_train[indices][:split_],ci_train[indices[split_:]]
    cv_train_, cv_valid = cv_train[indices][:split_],cv_train[indices[split_:]]
    ai_train_, ai_valid = ai_train[indices][:split_],ai_train[indices[split_:]]
    av_train_, av_valid = av_train[indices[:split_]],av_train[indices[split_:]]
    y_train_, y_valid = y_train[indices[:split_]], y_train[indices[split_:]] 

    # model selection
    """
    best_epoch = model.fit(ui_train_, uv_train_, ci_train_, cv_train_, ai_train_, av_train_, y_train_, ui_valid, uv_valid, ci_valid, cv_valid, ai_valid, av_valid, y_valid)
    params['epoch'] = best_epoch
    model = CFIN(**params)
    """ 
    # prediction results on test set
    
    import pdb; pdb.set_trace()
    # model.fit(ui_train, uv_train, ci_train, cv_train, ai_train, av_train, y_train, ui_test, uv_test, ci_test, cv_test, ai_test, av_test, y_test, early_stopping=False)
    model.saver.restore(model.sess, '../model/CFIN')
    model.evaluate(ui_test, uv_test, ci_test, cv_test, ai_test, av_test, y_test)   

# load data
dfTrain_u, dfTest_u, dfTrain_c, dfTest_c, dfTrain_a, dfTest_a = load_data()
dfTrain_a, dfTest_a, act_feat = feat_augment(dfTrain_a, dfTest_a)

params = {
    "embedding_size": 32,
    "attn_size": 16,
    "conv_size": 512,
    "context_size": 32,
    "deep_layers": [256],
    "dropout_deep": [0.9, 0.9],
    "dropout_attn": [1.],
    "activation": tf.nn.relu,
    "epoch": 27,
    "batch_size": 32,
    "learning_rate": 0.0001,
    "optimizer_type": "adam",
    "batch_norm": 1,
    "batch_norm_decay": 0.995,
    "l2_reg": 0.00001,
    "verbose": True,
    "attn": True,
    "eval_metric": roc_auc_score,
    "random_seed": 12345
}

y_train_dfm, y_test_dfm = model_run(dfTrain_u, dfTest_u, dfTrain_c, dfTest_c, dfTrain_a, dfTest_a,params, act_feat)

"""Training output:

-result mean AUC score
```
#params: 241281
[1] train-result=0.8433, valid-result=0.8353, valid-f1=0.8973 [100.0 s]
[2] train-result=0.8471, valid-result=0.8369, valid-f1=0.8979 [100.0 s]
[3] train-result=0.8497, valid-result=0.8402, valid-f1=0.8991 [97.2 s]
[4] train-result=0.8495, valid-result=0.8402, valid-f1=0.8993 [100.8 s]
[5] train-result=0.8530, valid-result=0.8447, valid-f1=0.9011 [98.1 s]
[6] train-result=0.8531, valid-result=0.8456, valid-f1=0.9012 [97.0 s]
[7] train-result=0.8544, valid-result=0.8462, valid-f1=0.9013 [96.8 s]
[8] train-result=0.8554, valid-result=0.8469, valid-f1=0.9015 [100.7 s]
[9] train-result=0.8554, valid-result=0.8468, valid-f1=0.9020 [98.9 s]
[10] train-result=0.8547, valid-result=0.8464, valid-f1=0.9021 [99.7 s]
[11] train-result=0.8574, valid-result=0.8485, valid-f1=0.9021 [101.2 s]
[12] train-result=0.8572, valid-result=0.8485, valid-f1=0.9015 [99.3 s]
[13] train-result=0.8573, valid-result=0.8488, valid-f1=0.9017 [99.0 s]
[14] train-result=0.8584, valid-result=0.8500, valid-f1=0.9017 [101.4 s]
[15] train-result=0.8596, valid-result=0.8483, valid-f1=0.9029 [98.6 s]
[16] train-result=0.8594, valid-result=0.8506, valid-f1=0.9034 [101.5 s]
[17] train-result=0.8592, valid-result=0.8503, valid-f1=0.9014 [103.8 s]
[18] train-result=0.8600, valid-result=0.8502, valid-f1=0.9018 [102.5 s]
[19] train-result=0.8609, valid-result=0.8506, valid-f1=0.9038 [101.2 s]
[20] train-result=0.8614, valid-result=0.8510, valid-f1=0.9030 [103.7 s]
[21] train-result=0.8606, valid-result=0.8513, valid-f1=0.9027 [101.6 s]
[22] train-result=0.8621, valid-result=0.8525, valid-f1=0.9030 [99.2 s]
[23] train-result=0.8616, valid-result=0.8511, valid-f1=0.9025 [100.2 s]
[24] train-result=0.8624, valid-result=0.8516, valid-f1=0.9031 [98.7 s]
[25] train-result=0.8632, valid-result=0.8520, valid-f1=0.9027 [99.3 s]
[26] train-result=0.8626, valid-result=0.8522, valid-f1=0.9035 [100.9 s]
[27] train-result=0.8633, valid-result=0.8522, valid-f1=0.9032 [96.7 s]
Save to path:  model/CFIN
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-5-f186e8d1d931> in <module>()
    161 }
    162 
--> 163 y_train_dfm, y_test_dfm = model_run(dfTrain_u, dfTest_u, dfTrain_c, dfTest_c, dfTrain_a, dfTest_a,params, act_feat)

TypeError: 'NoneType' object is not iterable
```

Test Output:
_res mean AUC score
```
(Pdb) test_res
0.8516378188128381
(Pdb) test_deep.shape
(19, 256)
(Pdb) test_f1
0.9031987785804723
```
"""

